


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
import plotly.graph_objects as go
import pickle
import os


keep_cols = [
    'basin',
    'source',
    'gps_height',
    'latitude',
    'longitude',
    'payment_type',
    'funder',
    'scheme_management',
    'quantity',
    'population',
    'amount_tsh',
    'installer',
    'public_meeting',
    'construction_year',
    'management',
    'extraction_type',
    'ward',
    'date_recorded',
    'permit',
    'water_quality'
]
dropped_cols = [
    'recorded_by',
    "waterpoint_type",
    "waterpoint_type_group",
    "source_type",
    "source_class",
    "quantity_group",
    'num_private',
    "management_group",
    "district_code",
    "quality_group",
    "payment",
    "extraction_type_group",
    "extraction_type_class",
    "scheme_name",
    "subvillage",
    "region_code",
    "wpt_name"
]
kept_categorical_features = [
    'funder',
    'installer',
    'basin',
    'ward',
    'recorded_by',
    'scheme_management',
    'extraction_type',
    'management',
    'payment_type',
    'water_quality',
    'quantity',
    'source'
]
missing_values_continuous = {
    'amount_tsh':0,
    'date_recorded':0,
    'gps_height':0,
    'longitude':0,
    'latitude':-2.000000e-08,
    'population':0,
    'construction_year':0,
}

missing_values_categorical = [
    'not known',
    'unknown',
    'none',
    '-',
    '##',
    'not kno',
    'unknown installer'
]
missing_values_by_feature = missing_values_continuous

for feature in kept_categorical_features:
    missing_values_by_feature[feature] = missing_values_categorical


# Function to determine the prediction type(used for visualization)
def prediction_type(row):
    if row['true_label'] == 'functional':
        return 'True Functional' if row['predicted_label'] == 'functional' else 'False Functional'
    elif row['true_label'] == 'non functional':
        return 'True Nonfunctional' if row['predicted_label'] == 'non functional' else 'False Nonfunctional'
    elif row['true_label'] == 'functional needs repair':
        return 'True FNR' if row['predicted_label'] == 'functional needs repair' else 'False FNR'

# prediction type colors(also used for visualization)
categories_colors = {
    'True Functional': 'green',
    'False Functional': 'lightgreen',
    'True Nonfunctional': 'blue',
    'False Nonfunctional': 'lightblue',
    'True FNR': 'purple',
    'False FNR': 'violet'
}

# Function for plotting on a map
def plot_predictions(X_data, true_labels, predicted_labels, title, categories_colors, scope, width, height):
    data_with_predictions = X_data.copy()
    data_with_predictions['true_label'] = true_labels
    data_with_predictions['predicted_label'] = predicted_labels
    data_with_predictions['pred_type'] = data_with_predictions.apply(prediction_type, axis=1)

    fig = go.Figure()
    for category, color in categories_colors.items():
        filtered_data = data_with_predictions[data_with_predictions['pred_type'] == category]
        fig.add_trace(go.Scattergeo(
            lon = filtered_data['longitude'],
            lat = filtered_data['latitude'],
            text = filtered_data['pred_type'],
            marker = dict(size = 4, color = color, line_color = 'rgb(40,40,40)', line_width = 0.5, sizemode = 'area'),
            name = category
        ))

    fig.update_layout(title=title, geo=dict(scope=scope, landcolor='rgb(217, 217, 217)'), width=width, height=height)
    fig.show()





# File paths for feather files
train_label_path_feather = "./data/train_label.feather"
train_data_path_feather = "./data/train_data.feather"
test_data_path_feather = "./data/test_data.feather"

try:

    # Read in pre-parsed feather files
    train_label_df = pd.read_feather(train_label_path_feather)
    train_data_df = pd.read_feather(train_data_path_feather)
    test_data_df = pd.read_feather(test_data_path_feather)

except FileNotFoundError:
    # File paths for the CSV files
    train_label_path = './data/train_label.csv'
    train_data_path = './data/train_data.csv'
    test_data_path = './data/test_data.csv'

    # Loading the CSV files into pandas DataFrames
    train_label_df = pd.read_csv(
        train_label_path,
        usecols=["status_group"],
        # dtype={"status_group":"category"},
        dtype_backend="pyarrow"
    )
    train_data_df = pd.read_csv(
        train_data_path,
        usecols=keep_cols,
        na_values=missing_values_by_feature,
        parse_dates=["date_recorded"],
        dtype_backend="pyarrow"
    )
    test_data_df = pd.read_csv(
        test_data_path,
        usecols=keep_cols,
        na_values=missing_values_by_feature,
        parse_dates=["date_recorded"],
        dtype_backend="pyarrow"
    )
    train_label_df.to_feather(train_label_path_feather)
    train_data_df.to_feather(train_data_path_feather)
    test_data_df.to_feather(test_data_path_feather)


train_label_df


train_data_df


train_data_df.dtypes


# Split training data to test model
X_train, X_test, y_train, y_test = train_test_split(
    train_data_df,
    train_label_df,
    test_size=0.2,
    random_state=42
)

# Check for missing values in the training set
print(X_train.isnull().sum())
print(X_train.shape)


train_data_df.dtypes





# Identify numerical and categorical columns
numeric_features = train_data_df.select_dtypes(exclude=['string','datetime', 'bool']).columns
categorical_features = train_data_df.select_dtypes(include=['string']).columns
all_features = train_data_df.select_dtypes(exclude=['datetime']).columns

print("Numeric:", numeric_features, sep="\n")
print("Categorical:", categorical_features, sep="\n")


categorical_features = ['funder', 'installer', 'basin', 'ward', 'scheme_management', 'extraction_type',
                         'management', 'payment_type', 'water_quality', 'quantity', 'source']

numeric_features = ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'population', 'construction_year']

boolean_features = ['public_meeting', 'permit']

# Create transformers for categorical, boolean, and numeric features
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(missing_values=pd.NA,strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore',drop='if_binary', min_frequency=0.01))
])

boolean_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent'))
])

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('imputer', KNNImputer(n_neighbors=3, weights='distance'))
])

# Create ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features),
        ('bool', boolean_transformer, boolean_features),
        ('num', numeric_transformer, numeric_features)
    ])

X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)


true_cats = []
for feat in categorical_features:
    uni = train_data_df[feat].unique()
    train_data_df[feat] = train_data_df[feat].fillna(train_data_df[feat].mode()[0])
    uni= [str(u) for u in uni if u is not pd.NA]
    if 1 < len(uni) <20:
        true_cats.append(feat)
        # if uni.dtype == 'string':
        # print(feat)
        # print(", ".join(uni))
        # print()
        # for cat in uni:
        #     print(cat, end=", ")
    else:
        print("Remove", feat, len(uni))
        print()


# sns.histplot(train_data_df[true_cats].select_dtypes(exclude=bool))
# order = train_data_df["quantity"].value_counts()
# order.sort_values()
# sns.catplot(x="quantity",kind="count", palette="ch:.25",order=order.index, data=train_data_df[true_cats].select_dtypes(exclude=bool))
# _ = plt.xticks(rotation=45)


sns.pairplot(data=pd.concat([pd.DataFrame(X_train, columns=numeric_features),train_label_df], axis=1), hue = "status_group")






feature_names = preprocessor.get_feature_names_out()  # Get feature names after preprocessing
class_names = ['Functional','Needs repair', 'Non-functional', ]


# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_preprocessed, y_train)

# Predict on validation set
y_test_pred_dt = dt_classifier.predict(X_test_preprocessed)



# Classification Report
print("Decision Tree Classifier Report:\n")
print(classification_report(y_test, y_test_pred_dt, target_names=class_names))

# Confusion Matrix with Percentages
fig = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_dt,
                                              normalize="true",
                                              display_labels=class_names,
                                              cmap='Blues',
                                              xticks_rotation=30,
                                              )


# Feature Importance Visualization

importances = dt_classifier.feature_importances_
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Decision Tree')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_dt, 'Decision Tree Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)





# Define the parameter grid
param_grid = {
    'n_estimators': [300, 400, 500],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42, verbose=2)
    
# Initialize GridSearchCV
grid_search_rf = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)
    
# Fit GridSearchCV to the training data
grid_search_rf.fit(X_train_preprocessed, y_train.values.ravel())
    
# Find the best parameters and use them to predict on the test set
best_rf_model = grid_search_rf.best_estimator_
y_test_pred_rf = best_rf_model.predict(X_test_preprocessed)
    
print("Best parameters found: ", best_rf_model)



# Classification Report
print("Random Forest Classifier Report:\n")
print(classification_report(y_test, y_test_pred_rf, target_names=class_names))

# Confusion Matrix for Random Forest
fig = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_rf,
                                              normalize="true",
                                              display_labels=class_names,
                                              cmap='Blues',
                                              xticks_rotation=30,
                                              )

# Feature Importance Visualization for Random Forest
importances_rf = rf_classifier.feature_importances_
importance_rf_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_rf}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_rf_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Random Forest')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_rf, 'Random Forest Water Pump Status Predictions', categories_colors, 'africa', 1000, 800)





X_train_preprocessed.shape


# Instantiate the Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier(
    n_estimators=500,
    learning_rate=0.07,
    max_depth=6,
    min_samples_split=10,
    random_state=12,
    subsample=0.5,
    n_iter_no_change=20,
    verbose=1,
    warm_start=True
)

# Fit the classifier to the training data
gb_classifier.fit(X_train_preprocessed, y_train.values.ravel())

# Predict on the validation set
y_test_pred_gb = gb_classifier.predict(X_test_preprocessed)


import joblib
joblib.dump(gb_classifier, "./models/gb_n(500)_lr(0.07)_md(6)_mss(10)_ss(0.5).pkl")


# Evaluation
print("Gradient Boosting Classifier Report:\n")
print(classification_report(y_test, y_test_pred_gb, target_names=class_names))

# Confusion Matrix with Percentages
fig = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_gb,
                                              normalize="true",
                                              display_labels=class_names,
                                              cmap='Blues',
                                              xticks_rotation=30,
                                              )

# Feature Importance Visualization
importances_gb = gb_classifier.feature_importances_
importance_gb_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_gb}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(8, 6))
sns.barplot(data=importance_gb_df.head(10),color='b', x='Importance', y='Feature')
plt.title('Top 10 Important Features in Gradient Boosting')
plt.show()


# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_gb, 'Gradient Boosting Water Pump Status Predictions', categories_colors, 'africa', 1000, 800)





# Initialize the SVM classifier
svm_classifier = SVC(kernel='rbf', C=1, random_state=42)  # Using RBF kernel and default C value

# Fit the classifier to the training data
svm_classifier.fit(X_train_preprocessed, y_train)

# Predict on the validation set
y_test_pred_svm = svm_classifier.predict(X_test_preprocessed)


# Evaluation Report
print("SVM Classifier Report:\n")
print(classification_report(y_test, y_test_pred_svm, target_names=class_names))

# Confusion Matrix
plt.figure(figsize=(8, 8))
cm_svm = confusion_matrix(y_test, y_test_pred_svm)
cm_svm_percentage = cm_svm.astype('float') / cm_svm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_svm_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('SVM Confusion Matrix')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_svm, 'SVM Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)





# Instantiate the KNN classifier(try varying n_neighbors)
knn_classifier = KNeighborsClassifier(weights='distance', n_jobs=-1)

# Define the parameter grid: try different values for n_neighbors
param_grid = {'n_neighbors': range(1, 15, 3)}

# Initialize CV
grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2)

# Fit the classifier to the training data
grid_search.fit(X_train_preprocessed, y_train)

# Find the best parameters
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best parameters found: ", best_params)
print("Best score found: ", best_score)

# Use the best parameters to predict on the validation set
y_test_pred_knn = grid_search.predict(X_test_preprocessed)


# Evaluation
print("KNN Classifier Report:\n")
print(classification_report(y_test, y_test_pred_knn, target_names=class_names))

# Confusion Matrix with Percentages
plt.figure(figsize=(8, 8))
cm_knn = confusion_matrix(y_test, y_test_pred_knn)
cm_knn_percentage = cm_knn.astype('float') / cm_knn.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_knn_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('KNN Confusion Matrix with Percentage')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_knn, 'KNN Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)


res = grid_search.cv_results_
param_scores = list((zip(res["rank_test_score"],res["mean_test_score"],res["params"], )))
param_scores.sort(key=lambda x: x[0])
param_scores



