


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
import plotly.graph_objects as go
import pickle
import os


keep_cols = [
    'basin',
    'source',
    'gps_height',
    'latitude',
    'longitude',
    'payment_type',
    'funder',
    'scheme_management',
    'quantity',
    'recorded_by',
    'population',
    'amount_tsh',
    'installer',
    'public_meeting',
    'construction_year',
    'management',
    'extraction_type',
    'ward',
    'date_recorded',
    'permit',
    'water_quality'
]
dropped_cols = [
    "waterpoint_type",
    "waterpoint_type_group",
    "source_type",
    "source_class",
    "quantity_group",
    'num_private',
    "management_group",
    "district_code",
    "quality_group",
    "payment",
    "extraction_type_group",
    "extraction_type_class",
    "scheme_name",
    "subvillage",
    "region_code",
    "wpt_name"
]
kept_categorical_features = [
    'funder',
    'installer',
    'basin',
    'ward',
    'recorded_by',
    'scheme_management',
    'extraction_type',
    'management',
    'payment_type',
    'water_quality',
    'quantity',
    'source'
]
missing_values_continuous = {
    'amount_tsh':0,
    'date_recorded':0,
    'gps_height':0,
    'longitude':0,
    'latitude':-2.000000e-08,
    'population':0,
    'construction_year':0,
}

missing_values_categorical = [
    'not known',
    'unknown',
    'none',
    '-',
    '##',
    'not kno',
    'unknown installer'
]
missing_values_by_feature = missing_values_continuous

for feature in kept_categorical_features:
    missing_values_by_feature[feature] = missing_values_categorical


# Function to determine the prediction type(used for visualization)
def prediction_type(row):
    if row['true_label'] == 'functional':
        return 'True Functional' if row['predicted_label'] == 'functional' else 'False Functional'
    elif row['true_label'] == 'non functional':
        return 'True Nonfunctional' if row['predicted_label'] == 'non functional' else 'False Nonfunctional'
    elif row['true_label'] == 'functional needs repair':
        return 'True FNR' if row['predicted_label'] == 'functional needs repair' else 'False FNR'

# prediction type colors(also used for visualization)
categories_colors = {
    'True Functional': 'green',
    'False Functional': 'lightgreen',
    'True Nonfunctional': 'blue',
    'False Nonfunctional': 'lightblue',
    'True FNR': 'purple',
    'False FNR': 'violet'
}

# Function for plotting on a map
def plot_predictions(X_data, true_labels, predicted_labels, title, categories_colors, scope, width, height):
    data_with_predictions = X_data.copy()
    data_with_predictions['true_label'] = true_labels
    data_with_predictions['predicted_label'] = predicted_labels
    data_with_predictions['pred_type'] = data_with_predictions.apply(prediction_type, axis=1)

    fig = go.Figure()
    for category, color in categories_colors.items():
        filtered_data = data_with_predictions[data_with_predictions['pred_type'] == category]
        fig.add_trace(go.Scattergeo(
            lon = filtered_data['longitude'],
            lat = filtered_data['latitude'],
            text = filtered_data['pred_type'],
            marker = dict(size = 4, color = color, line_color = 'rgb(40,40,40)', line_width = 0.5, sizemode = 'area'),
            name = category
        ))

    fig.update_layout(title=title, geo=dict(scope=scope, landcolor='rgb(217, 217, 217)'), width=width, height=height)
    fig.show()





# File paths for feather files
train_label_path_feather = "./data/train_label.feather"
train_data_path_feather = "./data/train_data.feather"
test_data_path_feather = "./data/test_data.feather"

try:

    # Read in pre-parsed feather files
    train_label_df = pd.read_feather(train_label_path_feather)
    train_data_df = pd.read_feather(train_data_path_feather)
    test_data_df = pd.read_feather(test_data_path_feather)

except FileNotFoundError:
    # File paths for the CSV files
    train_label_path = './data/train_label.csv'
    train_data_path = './data/train_data.csv'
    test_data_path = './data/test_data.csv'

    # Loading the CSV files into pandas DataFrames
    train_label_df = pd.read_csv(
        train_label_path,
        usecols=["status_group"],
        # dtype={"status_group":"category"},
        dtype_backend="pyarrow"
    )
    train_data_df = pd.read_csv(
        train_data_path,
        usecols=keep_cols,
        na_values=missing_values_by_feature,
        parse_dates=["date_recorded"],
        dtype_backend="pyarrow"
    )
    test_data_df = pd.read_csv(
        test_data_path,
        usecols=keep_cols,
        na_values=missing_values_by_feature,
        parse_dates=["date_recorded"],
        dtype_backend="pyarrow"
    )
    train_label_df.to_feather(train_label_path_feather)
    train_data_df.to_feather(train_data_path_feather)
    test_data_df.to_feather(test_data_path_feather)


train_label_df


train_data_df


# Split training data to test model
X_train, X_test, y_train, y_test = train_test_split(train_data_df, train_label_df, test_size=0.2, random_state=42)

# Check for missing values in the training set
print(X_train.isnull().sum())
print(X_train.shape)





# Identify numerical and categorical columns
numeric_features = train_data_df.select_dtypes(exclude=['string','datetime']).columns
categorical_features = train_data_df.select_dtypes(include=['string']).columns

# Create transformers for numerical and categorical features
numeric_transformer = Pipeline(
    steps=[
        ('imputer', KNNImputer(n_neighbors=3, weights='distance')),
        ('scaler', StandardScaler())
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(missing_values=pd.NA, strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]
)

# Combine transformers into a preprocessor step
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Apply preprocessing to training data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)





# Ensure that y_train and y_test are in the correct format
# If they are categorical, they should be converted to a consistent type (either all strings or all numeric)
y_train = y_train['status_group'].astype(str)
y_test = y_test['status_group'].astype(str)

# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_preprocessed, y_train)

# Predict on validation set
y_test_pred_dt = dt_classifier.predict(X_test_preprocessed)

# Replace 0, 1, 2 with class names
class_names = ['functional', 'non functional', 'functional needs repair']


# Classification Report
print("Decision Tree Classifier Report:\n")
print(classification_report(y_test, y_test_pred_dt, target_names=class_names))

# Confusion Matrix with Percentages
cm = confusion_matrix(y_test, y_test_pred_dt)
cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Decision Tree Confusion Matrix with Percentage')
plt.show()

# Feature Importance Visualization
feature_names = preprocessor.get_feature_names_out()  # Get feature names after preprocessing
importances = dt_classifier.feature_importances_
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Decision Tree')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_dt, 'Decision Tree Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)





# Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_preprocessed, y_train)

# Predict on validation set
y_test_pred_rf = rf_classifier.predict(X_test_preprocessed)



# Classification Report
print("Random Forest Classifier Report:\n")
print(classification_report(y_test, y_test_pred_rf, target_names=class_names))

# Confusion Matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_test_pred_rf)
cm_rf_percentage = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_rf_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Random Forest Confusion Matrix with Percentage')
plt.show()

# Feature Importance Visualization for Random Forest
importances_rf = rf_classifier.feature_importances_
importance_rf_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_rf}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_rf_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Random Forest')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_rf, 'Random Forest Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)





# Instantiate the Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42, subsample=0.3, verbose=1)

# Fit the classifier to the training data
gb_classifier.fit(X_train_preprocessed, y_train)

# Predict on the validation set
y_test_pred_gb = gb_classifier.predict(X_test_preprocessed)


# Evaluation
print("Gradient Boosting Classifier Report:\n")
print(classification_report(y_test, y_test_pred_gb, target_names=class_names))

# Confusion Matrix with Percentages
cm_gb = confusion_matrix(y_test, y_test_pred_gb)
cm_gb_percentage = cm_gb.astype('float') / cm_gb.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_gb_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Gradient Boosting Confusion Matrix with Percentage')
plt.show()

# Feature Importance Visualization
importances_gb = gb_classifier.feature_importances_
importance_gb_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_gb}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_gb_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Gradient Boosting')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_gb, 'Gradient Boosting Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)





# Initialize the SVM classifier
svm_classifier = SVC(kernel='rbf', C=1, random_state=42)  # Using RBF kernel and default C value

# Fit the classifier to the training data
svm_classifier.fit(X_train_preprocessed, y_train)

# Predict on the validation set
y_test_pred_svm = svm_classifier.predict(X_test_preprocessed)


# Evaluation Report
print("SVM Classifier Report:\n")
print(classification_report(y_test, y_test_pred_svm, target_names=class_names))

# Confusion Matrix
plt.figure(figsize=(8, 8))
cm_svm = confusion_matrix(y_test, y_test_pred_svm)
cm_svm_percentage = cm_svm.astype('float') / cm_svm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_svm_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('SVM Confusion Matrix')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_svm, 'SVM Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)





# Instantiate the KNN classifier(try varying n_neighbors)
knn_classifier = KNeighborsClassifier(weights='distance', n_jobs=-1)

# Define the parameter grid: try different values for n_neighbors
param_grid = {'n_neighbors': range(1, 15, 3)}

# Initialize CV
grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2)

# Fit the classifier to the training data
grid_search.fit(X_train_preprocessed, y_train)

# Find the best parameters
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best parameters found: ", best_params)
print("Best score found: ", best_score)

# Use the best parameters to predict on the validation set
y_test_pred_knn = grid_search.predict(X_test_preprocessed)


# Evaluation
print("KNN Classifier Report:\n")
print(classification_report(y_test, y_test_pred_knn, target_names=class_names))

# Confusion Matrix with Percentages
plt.figure(figsize=(8, 8))
cm_knn = confusion_matrix(y_test, y_test_pred_knn)
cm_knn_percentage = cm_knn.astype('float') / cm_knn.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_knn_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('KNN Confusion Matrix with Percentage')
plt.show()

# Plot on Map
plot_predictions(X_test, y_test, y_test_pred_knn, 'KNN Water Pump Status Predictions', categories_colors, 'africa', 1500, 1000)


res = grid_search.cv_results_
param_scores = list((zip(res["rank_test_score"],res["mean_test_score"],res["params"], )))
param_scores.sort(key=lambda x: x[0])
param_scores



