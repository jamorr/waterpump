


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
import pickle
import os





# File paths for the CSV files
train_label_path = './data/train_label.csv'
train_data_path = './data/train_data.csv'
test_data_path = './data/test_data.csv'
columns_path = './data/columns.csv'

# Loading the CSV files into pandas DataFrames
train_label_df = pd.read_csv(train_label_path)
train_data_df = pd.read_csv(train_data_path)
test_data_df = pd.read_csv(test_data_path)
columns_df = pd.read_csv(columns_path)


train_label_df


train_data_df


# Split training data to test model
X_train, X_test, y_train, y_test = train_test_split(train_data_df, train_label_df, test_size=0.2, random_state=42)

# Check for missing values in the training set
print(X_train.isnull().sum())
print(X_train.shape)





# Drop the 'scheme_name' column
X_train.drop('scheme_name', axis=1, inplace=True)
X_test.drop('scheme_name', axis=1, inplace=True)

# Identify numerical and categorical columns
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

# Create transformers for numerical and categorical features
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Combine transformers into a preprocessor step
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Apply preprocessing to training data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)





# Ensure that y_train and y_test are in the correct format
# If they are categorical, they should be converted to a consistent type (either all strings or all numeric)
y_train = y_train['status_group'].astype(str)
y_test = y_test['status_group'].astype(str)

# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_preprocessed, y_train)

# Predict on validation set
y_test_pred_dt = dt_classifier.predict(X_test_preprocessed)

# Replace 0, 1, 2 with class names
class_names = ['functional', 'non functional', 'functional needs repair']

# Classification Report
print("Decision Tree Classifier Report:\n")
print(classification_report(y_test, y_test_pred_dt, target_names=class_names))

# Confusion Matrix with Percentages
cm = confusion_matrix(y_test, y_test_pred_dt)
cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Decision Tree Confusion Matrix with Percentage')
plt.show()

# Feature Importance Visualization
feature_names = preprocessor.get_feature_names_out()  # Get feature names after preprocessing
importances = dt_classifier.feature_importances_
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Decision Tree')
plt.show()





# Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_preprocessed, y_train)

# Predict on validation set
y_test_pred_rf = rf_classifier.predict(X_test_preprocessed)


# Confusion Matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_test_pred_rf)
cm_rf_percentage = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_rf_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Random Forest Confusion Matrix with Percentage')
plt.show()

# Feature Importance Visualization for Random Forest
importances_rf = rf_classifier.feature_importances_
importance_rf_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_rf}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_rf_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Random Forest')
plt.show()





# Instantiate the Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=3, random_state=42)

# Fit the classifier to the training data
gb_classifier.fit(X_train_preprocessed, y_train)

# Predict on the validation set
y_test_pred_gb = gb_classifier.predict(X_test_preprocessed)

# Evaluation
print("Gradient Boosting Classifier Report:\n")
print(classification_report(y_test, y_test_pred_gb, target_names=class_names))

# Confusion Matrix with Percentages
cm_gb = confusion_matrix(y_test, y_test_pred_gb)
cm_gb_percentage = cm_gb.astype('float') / cm_gb.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_gb_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Gradient Boosting Confusion Matrix with Percentage')
plt.show()

# Feature Importance Visualization
importances_gb = gb_classifier.feature_importances_
importance_gb_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_gb}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_gb_df.head(10), x='Importance', y='Feature')
plt.title('Top 10 Important Features in Gradient Boosting')
plt.show()





# Instantiate the SVM classifier
svm_classifier = SVC(kernel='rbf', C=1, random_state=42)  # Using RBF kernel and default C value

# Fit the classifier to the training data
svm_classifier.fit(X_train_preprocessed, y_train)

# Predict on the validation set
y_test_pred_svm = svm_classifier.predict(X_test_preprocessed)

# Evaluation
print("SVM Classifier Report:\n")
print(classification_report(y_test, y_test_pred_svm, target_names=class_names))

# Confusion Matrix
cm_svm = confusion_matrix(y_test, y_test_pred_svm)
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('SVM Confusion Matrix')
plt.show()





# Instantiate the KNN classifier(try varying n_neighbors)
knn_classifier = KNeighborsClassifier(n_neighbors=5)

# Fit the classifier to the training data
knn_classifier.fit(X_train_preprocessed, y_train)

# Predict on the validation set
y_test_pred_knn = knn_classifier.predict(X_test_preprocessed)

# Evaluation
print("KNN Classifier Report:\n")
print(classification_report(y_test, y_test_pred_knn, target_names=class_names))

# Confusion Matrix with Percentages
cm_knn = confusion_matrix(y_test, y_test_pred_knn)
cm_knn_percentage = cm_knn.astype('float') / cm_knn.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_knn_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('KNN Confusion Matrix with Percentage')
plt.show()



